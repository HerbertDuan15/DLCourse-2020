{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"newSentimentClassification.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14inoUzrXaNElHw_xNvA77N-QZ8hNc_z-","authorship_tag":"ABX9TyN3/tmXYbFMx8WLlkH0lRln"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"S6wBlT6PMJSJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593212569512,"user_tz":-480,"elapsed":2435,"user":{"displayName":"HJ D","photoUrl":"","userId":"11566204124165679378"}},"outputId":"d664763f-0dcc-4fce-836c-8484eeeb5a82"},"source":["ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gg5JoF0v3pkC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593212606136,"user_tz":-480,"elapsed":922,"user":{"displayName":"HJ D","photoUrl":"","userId":"11566204124165679378"}},"outputId":"49a4fc49-5a18-47bc-ef70-f17e1d8f3b7c"},"source":["cd ./drive/My Drive/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fgbQUrxHMMR5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593226620795,"user_tz":-480,"elapsed":514408,"user":{"displayName":"HJ D","photoUrl":"","userId":"11566204124165679378"}},"outputId":"087a5842-e093-4485-d7f4-600cdffa1096"},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.utils.data as Data\n","import torchvision\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import gensim\n","#import gensim.models.word2vec as w2v\n","from sklearn.model_selection import train_test_split\n","import time\n","\n","\n","#数据预处理函数，返回文本与标签\n","def get_data(path):\n","#返回为文本，文本对应标签，文本数量\n","    data = []\n","    label = []\n","    nums = 0\n","    with open(path, 'r', encoding='UTF-8') as f:\n","        lines = f.readlines()\n","        text = ''\n","        for line in lines:\n","            try:\n","                label.append(torch.tensor(int(line[0]), dtype=torch.int64))\n","            except BaseException:  # 遇到首个字符不是标签的就跳过比如空行，并打印\n","                print('wrong line at:' + str(line) +  \",drop this line\")\n","                continue\n","            line_words = line.strip().split()[1:-1]  # 按照空字符\\t\\n 空格来切分\n","            text = line_words\n","            # print(text)\n","            data.append(text)\n","            nums = nums + 1\n","    return data, label, nums\n","#load word 2 vetc，加载词向量，可以事先预训练\n","def getw2v():\n","    model_file_name = 'wiki_word2vec_50.bin'\n","    # 模型训练，生成词向量\n","    '''\n","    sentences = w2v.LineSentence('trainword.txt')\n","    model = w2v.Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)\n","    model.save(model_file_name)\n","    '''\n","    # model = w2v.Word2Vec.load(model_file_name)\n","    model = gensim.models.KeyedVectors.load_word2vec_format(model_file_name, binary=True)\n","    return model;\n","\n","train_texts,train_labels,train_nums=get_data('./train.txt') #texts[0:train_nums]\n","valid_texts,valid_labels,valid_nums=get_data('./validation.txt') #texts[train_nums:train_nums+valid_nums]\n","test_texts,test_labels,test_nums=get_data('./test.txt')#texts[train_nums+valid_nums:train_nums+valid_nums+test_nums]\n","texts = train_texts + valid_texts + test_texts\n","labels = train_labels + valid_labels + test_labels\n","\n","sentence_length = [len(x) for x in texts] #长度\n","%matplotlib inline\n","#notebook\n","import matplotlib.pyplot as plt\n","plt.hist(sentence_length,max(sentence_length))\n","plt.xlim(0,max(sentence_length)/2)\n","plt.show()\n","#句子长度设为100就足够了\n","\n","# time.sleep(100)\n","# print(texts[1])\n","# print(texts[-1])\n","# time.sleep(1000)\n","#textCNN模型\n","class textCNN(nn.Module):\n","    def __init__(self,args):\n","        super(textCNN, self).__init__()\n","        vocb_size = args['vocb_size']\n","        dim = args['dim']\n","        n_class = args['n_class']\n","        max_len = args['max_len']\n","        embedding_matrix=args['embedding_matrix']\n","        #需要将事先训练好的词向量载入\n","        self.embeding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)\n","        self.conv1 = nn.Sequential(\n","                     nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,\n","                               stride=1, padding=2),\n","\n","                     nn.ReLU(),\n","                     nn.MaxPool2d(kernel_size=2) # (16,64,64)\n","                     )\n","        self.conv2 = nn.Sequential(\n","                     nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n","                     nn.ReLU(),\n","                     nn.MaxPool2d(2)\n","                     )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        self.conv4 = nn.Sequential(  # (16,64,64)\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        self.fc1 = nn.Linear(2304, 512)\n","        self.fc2 = nn.Linear(512, 100)\n","        self.out = nn.Linear(100, n_class)\n","\n","    def forward(self, x):\n","        x = self.embeding(x)\n","        x=x.view(x.size(0),1,max_len,word_dim)\n","        #print(x.size())\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = x.view(x.size(0), -1) # 将（batch，outchanel,w,h）展平为（batch，outchanel*w*h）\n","        # print(x.size())\n","        output = self.fc1(x)\n","        output = self.fc2(output)\n","        output = self.out(output)\n","        return output\n","\n","#此处统计了所有词的次数\n","word_vocb=[]\n","word_vocb.append('')\n","for text in texts:\n","    for word in text:\n","        word_vocb.append(word)\n","word_vocb=set(word_vocb)#使用set去重\n","vocb_size=len(word_vocb)#得到了出现的不同的词的个数 58463 个\n","# print(vocb_size) #58436\n","# time.sleep(1000)\n","#设置词表大小\n","nb_words=40000\n","if nb_words<vocb_size:\n","    nb_words=vocb_size\n","max_len=100;#限定的每个句子的长度，超过截取，小于补0\n","word_dim=50;#每个词vector的维度\n","n_class=2;#最后的分类\n","\n","\n","args={}\n","#textCNN调用的参数\n","args['vocb_size']=nb_words#词表大小\n","args['max_len']=max_len\n","args['n_class']=n_class\n","args['dim']=word_dim\n","\n","\n","#词表与索引的map\n","word_to_idx={word:i for i,word in enumerate(word_vocb)}\n","idx_to_word={word_to_idx[word]:word for word in word_to_idx}\n","#每个单词的对应的词向量\n","embeddings_index = getw2v()\n","#预先处理好的词向量\n","embedding_matrix = np.zeros((nb_words, word_dim))\n","for word, i in word_to_idx.items():\n","    if i >= nb_words:\n","        continue\n","    if word in embeddings_index:\n","        embedding_vector = embeddings_index[word]\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n","args['embedding_matrix']=torch.Tensor(embedding_matrix)\n","\n","#生成训练数据，需要将训练数据的Word转换为word的索引\n","texts_with_id=np.zeros([len(texts),max_len]) # 训练句子总数 * 句子长度的矩阵\n","for i in range(0,len(texts)):#第i句影评\n","    if len(texts[i])<max_len:#句子长度比max_len小\n","        for j in range(0,len(texts[i])):#其中第j个词\n","            texts_with_id[i][j]=word_to_idx[texts[i][j]]\n","        for j in range(len(texts[i]),max_len):\n","            texts_with_id[i][j] = word_to_idx['']\n","    else:\n","        for j in range(0,max_len):\n","            texts_with_id[i][j]=word_to_idx[texts[i][j]]\n","\n","#构建textCNN模型\n","cnn=textCNN(args)\n","print(cnn)\n","\n","EPOCH=3\n","LR = 0.001\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n","#损失函数\n","loss_function = nn.CrossEntropyLoss()\n","#训练批次大小\n","batch_size=100;\n","#划分训练数据和测试数据\n","x_train, x_test, y_train, y_test = train_test_split(texts_with_id, labels, test_size=0.1, random_state = int(time.time()), shuffle = True)\n","\n","test_x=torch.LongTensor(x_test)\n","test_y=torch.LongTensor(y_test)\n","train_x=x_train\n","train_y=y_train\n","\n","test_batch_size=100;\n","#开始训练\n","for epoch in range(EPOCH):\n","    for i in range(0,(int)(len(train_x)/batch_size)):\n","        b_x = Variable(torch.LongTensor(train_x[i*batch_size:i*batch_size+batch_size]))\n","        b_y = Variable(torch.LongTensor((train_y[i*batch_size:i*batch_size+batch_size])))\n","        output = cnn(b_x)\n","        loss = loss_function(output, b_y)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step() \n","        \n","        pred_y = torch.max(output, 1)[1].data.squeeze()\n","        acc = (b_y == pred_y)\n","        accuracy = acc.numpy().sum() / b_y.size(0)\n","        if(i+1)%10 == 0:\n","          print('TRAIN:EPOCH [%d/%d],batch[%d/%d],train loss is: %s, now accuracy is: %s\\n'%(epoch+1,EPOCH,i+1,len(train_x)/batch_size,str(loss.item()),str(accuracy)))\n","    \n","    # 测试\n","    test_all = 0\n","    test_right = 0\n","    for j in range(0, (int)(len(test_x) / test_batch_size)):\n","        b_x = Variable(torch.LongTensor(test_x[j * test_batch_size:j * test_batch_size + test_batch_size]))\n","        b_y = Variable(torch.LongTensor((test_y[j * test_batch_size:j * test_batch_size + test_batch_size])))\n","        test_output = cnn(b_x)\n","        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n","        acc = (pred_y == b_y)\n","        acc = acc.numpy().sum()\n","        now_acc = acc / b_y.size(0)\n","        test_right = test_right + acc\n","        test_all = test_all + b_y.size(0)\n","        total_acc = test_right / test_all\n","        print('TEST:in batch[%d/%d] accuracy is: %s, total accuracy is: %s\\n'%(j+1,len(test_x)/test_batch_size,str(now_acc),str(total_acc)))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["wrong line at:\n",",drop this line\n","wrong line at:\n",",drop this line\n","wrong line at:\n",",drop this line\n","wrong line at:\n",",drop this line\n","wrong line at:\n",",drop this line\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["textCNN(\n","  (embeding): Embedding(58463, 50)\n","  (conv1): Sequential(\n","    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv3): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv4): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (fc1): Linear(in_features=2304, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=100, bias=True)\n","  (out): Linear(in_features=100, out_features=2, bias=True)\n",")\n","TRAIN:EPOCH [1/3],batch[10/233],train loss is: 0.6937283277511597, now accuracy is: 0.48\n","\n","TRAIN:EPOCH [1/3],batch[20/233],train loss is: 0.6969062089920044, now accuracy is: 0.4\n","\n","TRAIN:EPOCH [1/3],batch[30/233],train loss is: 0.6927421689033508, now accuracy is: 0.51\n","\n","TRAIN:EPOCH [1/3],batch[40/233],train loss is: 0.6935127377510071, now accuracy is: 0.44\n","\n","TRAIN:EPOCH [1/3],batch[50/233],train loss is: 0.6899034976959229, now accuracy is: 0.54\n","\n","TRAIN:EPOCH [1/3],batch[60/233],train loss is: 0.6947702169418335, now accuracy is: 0.48\n","\n","TRAIN:EPOCH [1/3],batch[70/233],train loss is: 0.6960458159446716, now accuracy is: 0.48\n","\n","TRAIN:EPOCH [1/3],batch[80/233],train loss is: 0.6820248365402222, now accuracy is: 0.59\n","\n","TRAIN:EPOCH [1/3],batch[90/233],train loss is: 0.674338698387146, now accuracy is: 0.65\n","\n","TRAIN:EPOCH [1/3],batch[100/233],train loss is: 0.6135892271995544, now accuracy is: 0.78\n","\n","TRAIN:EPOCH [1/3],batch[110/233],train loss is: 0.6178832054138184, now accuracy is: 0.66\n","\n","TRAIN:EPOCH [1/3],batch[120/233],train loss is: 0.4973001182079315, now accuracy is: 0.71\n","\n","TRAIN:EPOCH [1/3],batch[130/233],train loss is: 0.5051260590553284, now accuracy is: 0.75\n","\n","TRAIN:EPOCH [1/3],batch[140/233],train loss is: 0.5307581424713135, now accuracy is: 0.77\n","\n","TRAIN:EPOCH [1/3],batch[150/233],train loss is: 0.5056383609771729, now accuracy is: 0.76\n","\n","TRAIN:EPOCH [1/3],batch[160/233],train loss is: 0.513614296913147, now accuracy is: 0.75\n","\n","TRAIN:EPOCH [1/3],batch[170/233],train loss is: 0.4485389292240143, now accuracy is: 0.8\n","\n","TRAIN:EPOCH [1/3],batch[180/233],train loss is: 0.5149226188659668, now accuracy is: 0.74\n","\n","TRAIN:EPOCH [1/3],batch[190/233],train loss is: 0.4338513910770416, now accuracy is: 0.78\n","\n","TRAIN:EPOCH [1/3],batch[200/233],train loss is: 0.43251660466194153, now accuracy is: 0.79\n","\n","TRAIN:EPOCH [1/3],batch[210/233],train loss is: 0.5486344695091248, now accuracy is: 0.7\n","\n","TRAIN:EPOCH [1/3],batch[220/233],train loss is: 0.466712087392807, now accuracy is: 0.84\n","\n","TRAIN:EPOCH [1/3],batch[230/233],train loss is: 0.5183259844779968, now accuracy is: 0.77\n","\n","TEST:in batch[1/26] accuracy is: 0.74, total accuracy is: 0.74\n","\n","TEST:in batch[2/26] accuracy is: 0.86, total accuracy is: 0.8\n","\n","TEST:in batch[3/26] accuracy is: 0.73, total accuracy is: 0.7766666666666666\n","\n","TEST:in batch[4/26] accuracy is: 0.78, total accuracy is: 0.7775\n","\n","TEST:in batch[5/26] accuracy is: 0.79, total accuracy is: 0.78\n","\n","TEST:in batch[6/26] accuracy is: 0.83, total accuracy is: 0.7883333333333333\n","\n","TEST:in batch[7/26] accuracy is: 0.84, total accuracy is: 0.7957142857142857\n","\n","TEST:in batch[8/26] accuracy is: 0.85, total accuracy is: 0.8025\n","\n","TEST:in batch[9/26] accuracy is: 0.8, total accuracy is: 0.8022222222222222\n","\n","TEST:in batch[10/26] accuracy is: 0.75, total accuracy is: 0.797\n","\n","TEST:in batch[11/26] accuracy is: 0.84, total accuracy is: 0.8009090909090909\n","\n","TEST:in batch[12/26] accuracy is: 0.79, total accuracy is: 0.8\n","\n","TEST:in batch[13/26] accuracy is: 0.77, total accuracy is: 0.7976923076923077\n","\n","TEST:in batch[14/26] accuracy is: 0.85, total accuracy is: 0.8014285714285714\n","\n","TEST:in batch[15/26] accuracy is: 0.83, total accuracy is: 0.8033333333333333\n","\n","TEST:in batch[16/26] accuracy is: 0.8, total accuracy is: 0.803125\n","\n","TEST:in batch[17/26] accuracy is: 0.77, total accuracy is: 0.8011764705882353\n","\n","TEST:in batch[18/26] accuracy is: 0.81, total accuracy is: 0.8016666666666666\n","\n","TEST:in batch[19/26] accuracy is: 0.75, total accuracy is: 0.7989473684210526\n","\n","TEST:in batch[20/26] accuracy is: 0.79, total accuracy is: 0.7985\n","\n","TEST:in batch[21/26] accuracy is: 0.84, total accuracy is: 0.8004761904761905\n","\n","TEST:in batch[22/26] accuracy is: 0.85, total accuracy is: 0.8027272727272727\n","\n","TEST:in batch[23/26] accuracy is: 0.79, total accuracy is: 0.8021739130434783\n","\n","TEST:in batch[24/26] accuracy is: 0.77, total accuracy is: 0.8008333333333333\n","\n","TEST:in batch[25/26] accuracy is: 0.79, total accuracy is: 0.8004\n","\n","TEST:in batch[26/26] accuracy is: 0.77, total accuracy is: 0.7992307692307692\n","\n","TRAIN:EPOCH [2/3],batch[10/233],train loss is: 0.47175243496894836, now accuracy is: 0.82\n","\n","TRAIN:EPOCH [2/3],batch[20/233],train loss is: 0.49412649869918823, now accuracy is: 0.8\n","\n","TRAIN:EPOCH [2/3],batch[30/233],train loss is: 0.3969680666923523, now accuracy is: 0.82\n","\n","TRAIN:EPOCH [2/3],batch[40/233],train loss is: 0.5352533459663391, now accuracy is: 0.78\n","\n","TRAIN:EPOCH [2/3],batch[50/233],train loss is: 0.42319273948669434, now accuracy is: 0.81\n","\n","TRAIN:EPOCH [2/3],batch[60/233],train loss is: 0.4194364845752716, now accuracy is: 0.82\n","\n","TRAIN:EPOCH [2/3],batch[70/233],train loss is: 0.22364671528339386, now accuracy is: 0.95\n","\n","TRAIN:EPOCH [2/3],batch[80/233],train loss is: 0.4574633836746216, now accuracy is: 0.8\n","\n","TRAIN:EPOCH [2/3],batch[90/233],train loss is: 0.33123648166656494, now accuracy is: 0.83\n","\n","TRAIN:EPOCH [2/3],batch[100/233],train loss is: 0.2368125021457672, now accuracy is: 0.91\n","\n","TRAIN:EPOCH [2/3],batch[110/233],train loss is: 0.3544730246067047, now accuracy is: 0.86\n","\n","TRAIN:EPOCH [2/3],batch[120/233],train loss is: 0.2602457106113434, now accuracy is: 0.88\n","\n","TRAIN:EPOCH [2/3],batch[130/233],train loss is: 0.30987349152565, now accuracy is: 0.87\n","\n","TRAIN:EPOCH [2/3],batch[140/233],train loss is: 0.3271538019180298, now accuracy is: 0.89\n","\n","TRAIN:EPOCH [2/3],batch[150/233],train loss is: 0.2755694091320038, now accuracy is: 0.9\n","\n","TRAIN:EPOCH [2/3],batch[160/233],train loss is: 0.35959845781326294, now accuracy is: 0.8\n","\n","TRAIN:EPOCH [2/3],batch[170/233],train loss is: 0.38651716709136963, now accuracy is: 0.86\n","\n","TRAIN:EPOCH [2/3],batch[180/233],train loss is: 0.27728328108787537, now accuracy is: 0.88\n","\n","TRAIN:EPOCH [2/3],batch[190/233],train loss is: 0.3507099449634552, now accuracy is: 0.85\n","\n","TRAIN:EPOCH [2/3],batch[200/233],train loss is: 0.29531821608543396, now accuracy is: 0.89\n","\n","TRAIN:EPOCH [2/3],batch[210/233],train loss is: 0.39246857166290283, now accuracy is: 0.87\n","\n","TRAIN:EPOCH [2/3],batch[220/233],train loss is: 0.334163099527359, now accuracy is: 0.85\n","\n","TRAIN:EPOCH [2/3],batch[230/233],train loss is: 0.29292863607406616, now accuracy is: 0.87\n","\n","TEST:in batch[1/26] accuracy is: 0.76, total accuracy is: 0.76\n","\n","TEST:in batch[2/26] accuracy is: 0.86, total accuracy is: 0.81\n","\n","TEST:in batch[3/26] accuracy is: 0.77, total accuracy is: 0.7966666666666666\n","\n","TEST:in batch[4/26] accuracy is: 0.84, total accuracy is: 0.8075\n","\n","TEST:in batch[5/26] accuracy is: 0.84, total accuracy is: 0.814\n","\n","TEST:in batch[6/26] accuracy is: 0.86, total accuracy is: 0.8216666666666667\n","\n","TEST:in batch[7/26] accuracy is: 0.87, total accuracy is: 0.8285714285714286\n","\n","TEST:in batch[8/26] accuracy is: 0.87, total accuracy is: 0.83375\n","\n","TEST:in batch[9/26] accuracy is: 0.83, total accuracy is: 0.8333333333333334\n","\n","TEST:in batch[10/26] accuracy is: 0.82, total accuracy is: 0.832\n","\n","TEST:in batch[11/26] accuracy is: 0.82, total accuracy is: 0.8309090909090909\n","\n","TEST:in batch[12/26] accuracy is: 0.84, total accuracy is: 0.8316666666666667\n","\n","TEST:in batch[13/26] accuracy is: 0.81, total accuracy is: 0.83\n","\n","TEST:in batch[14/26] accuracy is: 0.87, total accuracy is: 0.8328571428571429\n","\n","TEST:in batch[15/26] accuracy is: 0.84, total accuracy is: 0.8333333333333334\n","\n","TEST:in batch[16/26] accuracy is: 0.8, total accuracy is: 0.83125\n","\n","TEST:in batch[17/26] accuracy is: 0.82, total accuracy is: 0.8305882352941176\n","\n","TEST:in batch[18/26] accuracy is: 0.84, total accuracy is: 0.8311111111111111\n","\n","TEST:in batch[19/26] accuracy is: 0.84, total accuracy is: 0.8315789473684211\n","\n","TEST:in batch[20/26] accuracy is: 0.83, total accuracy is: 0.8315\n","\n","TEST:in batch[21/26] accuracy is: 0.89, total accuracy is: 0.8342857142857143\n","\n","TEST:in batch[22/26] accuracy is: 0.92, total accuracy is: 0.8381818181818181\n","\n","TEST:in batch[23/26] accuracy is: 0.81, total accuracy is: 0.8369565217391305\n","\n","TEST:in batch[24/26] accuracy is: 0.8, total accuracy is: 0.8354166666666667\n","\n","TEST:in batch[25/26] accuracy is: 0.84, total accuracy is: 0.8356\n","\n","TEST:in batch[26/26] accuracy is: 0.82, total accuracy is: 0.835\n","\n","TRAIN:EPOCH [3/3],batch[10/233],train loss is: 0.30369484424591064, now accuracy is: 0.9\n","\n","TRAIN:EPOCH [3/3],batch[20/233],train loss is: 0.32977399230003357, now accuracy is: 0.87\n","\n","TRAIN:EPOCH [3/3],batch[30/233],train loss is: 0.23866413533687592, now accuracy is: 0.92\n","\n","TRAIN:EPOCH [3/3],batch[40/233],train loss is: 0.39209064841270447, now accuracy is: 0.85\n","\n","TRAIN:EPOCH [3/3],batch[50/233],train loss is: 0.32857224345207214, now accuracy is: 0.85\n","\n","TRAIN:EPOCH [3/3],batch[60/233],train loss is: 0.27795279026031494, now accuracy is: 0.87\n","\n","TRAIN:EPOCH [3/3],batch[70/233],train loss is: 0.1097576916217804, now accuracy is: 0.98\n","\n","TRAIN:EPOCH [3/3],batch[80/233],train loss is: 0.33251652121543884, now accuracy is: 0.89\n","\n","TRAIN:EPOCH [3/3],batch[90/233],train loss is: 0.22670720517635345, now accuracy is: 0.92\n","\n","TRAIN:EPOCH [3/3],batch[100/233],train loss is: 0.11233361065387726, now accuracy is: 0.97\n","\n","TRAIN:EPOCH [3/3],batch[110/233],train loss is: 0.22163204848766327, now accuracy is: 0.89\n","\n","TRAIN:EPOCH [3/3],batch[120/233],train loss is: 0.17236606776714325, now accuracy is: 0.93\n","\n","TRAIN:EPOCH [3/3],batch[130/233],train loss is: 0.17959044873714447, now accuracy is: 0.92\n","\n","TRAIN:EPOCH [3/3],batch[140/233],train loss is: 0.27104651927948, now accuracy is: 0.93\n","\n","TRAIN:EPOCH [3/3],batch[150/233],train loss is: 0.1784396767616272, now accuracy is: 0.92\n","\n","TRAIN:EPOCH [3/3],batch[160/233],train loss is: 0.1748771071434021, now accuracy is: 0.96\n","\n","TRAIN:EPOCH [3/3],batch[170/233],train loss is: 0.3362520635128021, now accuracy is: 0.89\n","\n","TRAIN:EPOCH [3/3],batch[180/233],train loss is: 0.10871785879135132, now accuracy is: 0.98\n","\n","TRAIN:EPOCH [3/3],batch[190/233],train loss is: 0.277923047542572, now accuracy is: 0.89\n","\n","TRAIN:EPOCH [3/3],batch[200/233],train loss is: 0.18454867601394653, now accuracy is: 0.95\n","\n","TRAIN:EPOCH [3/3],batch[210/233],train loss is: 0.2693216800689697, now accuracy is: 0.92\n","\n","TRAIN:EPOCH [3/3],batch[220/233],train loss is: 0.22866123914718628, now accuracy is: 0.92\n","\n","TRAIN:EPOCH [3/3],batch[230/233],train loss is: 0.10473629087209702, now accuracy is: 0.98\n","\n","TEST:in batch[1/26] accuracy is: 0.79, total accuracy is: 0.79\n","\n","TEST:in batch[2/26] accuracy is: 0.84, total accuracy is: 0.815\n","\n","TEST:in batch[3/26] accuracy is: 0.77, total accuracy is: 0.8\n","\n","TEST:in batch[4/26] accuracy is: 0.83, total accuracy is: 0.8075\n","\n","TEST:in batch[5/26] accuracy is: 0.85, total accuracy is: 0.816\n","\n","TEST:in batch[6/26] accuracy is: 0.89, total accuracy is: 0.8283333333333334\n","\n","TEST:in batch[7/26] accuracy is: 0.88, total accuracy is: 0.8357142857142857\n","\n","TEST:in batch[8/26] accuracy is: 0.86, total accuracy is: 0.83875\n","\n","TEST:in batch[9/26] accuracy is: 0.78, total accuracy is: 0.8322222222222222\n","\n","TEST:in batch[10/26] accuracy is: 0.87, total accuracy is: 0.836\n","\n","TEST:in batch[11/26] accuracy is: 0.84, total accuracy is: 0.8363636363636363\n","\n","TEST:in batch[12/26] accuracy is: 0.84, total accuracy is: 0.8366666666666667\n","\n","TEST:in batch[13/26] accuracy is: 0.82, total accuracy is: 0.8353846153846154\n","\n","TEST:in batch[14/26] accuracy is: 0.85, total accuracy is: 0.8364285714285714\n","\n","TEST:in batch[15/26] accuracy is: 0.85, total accuracy is: 0.8373333333333334\n","\n","TEST:in batch[16/26] accuracy is: 0.8, total accuracy is: 0.835\n","\n","TEST:in batch[17/26] accuracy is: 0.82, total accuracy is: 0.8341176470588235\n","\n","TEST:in batch[18/26] accuracy is: 0.85, total accuracy is: 0.835\n","\n","TEST:in batch[19/26] accuracy is: 0.84, total accuracy is: 0.8352631578947368\n","\n","TEST:in batch[20/26] accuracy is: 0.83, total accuracy is: 0.835\n","\n","TEST:in batch[21/26] accuracy is: 0.86, total accuracy is: 0.8361904761904762\n","\n","TEST:in batch[22/26] accuracy is: 0.9, total accuracy is: 0.8390909090909091\n","\n","TEST:in batch[23/26] accuracy is: 0.82, total accuracy is: 0.8382608695652174\n","\n","TEST:in batch[24/26] accuracy is: 0.82, total accuracy is: 0.8375\n","\n","TEST:in batch[25/26] accuracy is: 0.84, total accuracy is: 0.8376\n","\n","TEST:in batch[26/26] accuracy is: 0.83, total accuracy is: 0.8373076923076923\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tLqSejQaY5db","colab_type":"text"},"source":["经过测试，发现EPOCH为3时就可以了，再大可能就会产生过拟合"]}]}